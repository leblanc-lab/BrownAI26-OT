{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1815ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab. Assuming local environment.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# GOOGLE COLAB SETUP\n",
    "# ---------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Detected Google Colab environment.\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Smart Installation Check\n",
    "    # ---------------------------------------------------------\n",
    "    # We check the installed NumPy version via pip *before* deciding to install.\n",
    "    # This prevents the cell from hanging on re-runs after a restart.\n",
    "    \n",
    "    current_numpy_version = \"0.0.0\"\n",
    "    try:\n",
    "        # Run 'pip show numpy' to get the version on disk\n",
    "        res = subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"numpy\"], capture_output=True, text=True)\n",
    "        m = re.search(r\"Version:\\s*(\\d+\\.\\d+\\.\\d+)\", res.stdout)\n",
    "        if m:\n",
    "            current_numpy_version = m.group(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"Current NumPy version on disk: {current_numpy_version}\")\n",
    "    \n",
    "    # Determine if we need to install/downgrade\n",
    "    needs_install = False\n",
    "    \n",
    "    # Condition 1: NumPy must be 1.x\n",
    "    if not current_numpy_version.startswith(\"1.\"):\n",
    "        print(\"NumPy 2.x (or unknown) detected. Downgrade required.\")\n",
    "        needs_install = True\n",
    "    else:\n",
    "        # Condition 2: Check if pyshaper is installed\n",
    "        res_pkg = subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"pyshaper\"], capture_output=True, text=True)\n",
    "        if \"Name: pyshaper\" not in res_pkg.stdout:\n",
    "            print(\"pyshaper not found. Installation required.\")\n",
    "            needs_install = True\n",
    "\n",
    "    if needs_install:\n",
    "        print(\"Installing required packages (this may take a minute)...\")\n",
    "        # AGGRESSIVE COMPATIBILITY FIX:\n",
    "        # Force reinstall to ensure we get numpy<2.0.0 and compatible scipy/sklearn\n",
    "        !pip install -q -U --force-reinstall \"numpy<2.0.0\" \"scipy<1.13.0\" \"scikit-learn<1.5.0\" energyflow pot uproot awkward vector pyshaper\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INSTALLATION COMPLETE.\")\n",
    "        print(\"CRITICAL: You MUST restart the Colab runtime now.\")\n",
    "        print(\"1. Go to menu: Runtime > Restart session\")\n",
    "        print(\"2. Run this cell again (it will skip installation next time)\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"Environment appears correct (NumPy 1.x installed). Skipping installation.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Download Dataset\n",
    "    # ---------------------------------------------------------\n",
    "    folder_id = '13Rs54cAg-MyUocZMLRRozjeGcRdfjDJ5'\n",
    "    output_folder = 'jetclass'\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        print(f\"Downloading dataset to {output_folder}/...\")\n",
    "        !pip install -q -U --no-cache-dir gdown --pre\n",
    "        import gdown\n",
    "        gdown.download_folder(id=folder_id, output=output_folder, quiet=False)\n",
    "    else:\n",
    "        print(f\"Folder '{output_folder}' already exists. Skipping download.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Not running in Google Colab. Assuming local environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce5b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Libraries imported successfully.\n",
      "EnergyFlow version: 1.4.0\n",
      "Uproot version: 5.6.9\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CRITICAL: Check NumPy version before importing other libraries\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "if np.__version__ >= '2.0.0':\n",
    "    msg = (\n",
    "        f\"Detected NumPy {np.__version__}, but this notebook requires NumPy < 2.0.0.\\n\"\n",
    "        \"Please run the 'GOOGLE COLAB SETUP' cell above to install the correct versions,\\n\"\n",
    "        \"then RESTART the runtime (Runtime > Restart session) and run this cell again.\"\n",
    "    )\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "import ot\n",
    "import uproot\n",
    "import energyflow as ef\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"EnergyFlow version: {ef.__version__}\")\n",
    "print(f\"Uproot version: {uproot.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9e2a1",
   "metadata": {},
   "source": [
    "## 1. Data Loading: JetClass Dataset\n",
    "\n",
    "The [JetClass dataset](https://zenodo.org/records/6619768) is a large-scale dataset for jet tagging. We will load the data from ROOT files located in the `inputs/` directory.\n",
    "\n",
    "*   **QCD Jets:** Background jets, typically 1-prong.\n",
    "*   **W Jets:** Signal jets, 2-prong structure.\n",
    "*   **Top Jets:** Signal jets, 3-prong structure.\n",
    "\n",
    "We will use `uproot` to load the particle kinematics and convert them into the $(\\eta, \\phi)$ plane for Optimal Transport analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13463eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# Visualization & Utility Functions\n",
    "\n",
    "def plot_jet(X, w, title=\"Jet\"):\n",
    "    \"\"\"\n",
    "    Plot a single jet as a scatter plot in the eta-phi plane.\n",
    "    \"\"\"\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=w*1000, alpha=0.6, c=w, cmap='viridis')\n",
    "    plt.xlim(-1.2, 1.2)\n",
    "    plt.ylim(-1.2, 1.2)\n",
    "    plt.xlabel(\"$\\\\eta$\")\n",
    "    plt.ylabel(\"$\\\\phi$\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.colorbar(label=\"$p_T$ fraction\")\n",
    "\n",
    "def jet_to_image(X, w, grid_size=28, r=1.2):\n",
    "    \"\"\"\n",
    "    Rasterize a jet (point cloud) into a grid image.\n",
    "    \"\"\"\n",
    "    img, _, _ = np.histogram2d(\n",
    "        X[:, 0], X[:, 1], \n",
    "        bins=grid_size, \n",
    "        range=[[-r, r], [-r, r]], \n",
    "        weights=w\n",
    "    )\n",
    "    return img.T  # Transpose to match image coordinates\n",
    "\n",
    "print(\"Visualization utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6efb65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected. Using CPU.\n",
      "Found 10 ROOT files. Loading...\n",
      "Loading TTbar->bqq from TTBar_000.root...\n",
      "Loading W->qq from WToQQ_000.root...\n",
      "Loading Z->qq from ZToQQ_000.root...\n",
      "Total jets loaded: 3000\n",
      "Computing pairwise Sinkhorn (CPU) distances using CPU parallel processing...\n",
      "Total pairs to compute: 4498500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 274\u001b[39m\n\u001b[32m    271\u001b[39m pairs = [(i, j) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i+\u001b[32m1\u001b[39m, N)]\n\u001b[32m    272\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal pairs to compute: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_cores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_ot_pair_cpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m D = np.zeros((N, N))\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, j, val \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/teaching/Spring2026_CFPU_AISchool/venv/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/teaching/Spring2026_CFPU_AISchool/venv/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/teaching/Spring2026_CFPU_AISchool/venv/lib/python3.11/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# EXPERIMENTS: Multi-class jets (JetClass inputs), OT distances via Sinkhorn or EnergyFlow\n",
    "# ---------------------------------------------------------\n",
    "import os\n",
    "import glob\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Try to import GPU libraries\n",
    "try:\n",
    "    import torch\n",
    "    import geomloss\n",
    "    USE_GPU = torch.cuda.is_available()\n",
    "    if USE_GPU:\n",
    "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"No GPU detected. Using CPU.\")\n",
    "except ImportError:\n",
    "    USE_GPU = False\n",
    "    print(\"Torch/GeomLoss not found. Using CPU.\")\n",
    "\n",
    "# Parameters: 10 JetClass categories\n",
    "classes = {\n",
    "    0: { 'name': 'W->qq',        'file_pattern': 'WToQQ' },\n",
    "    1: { 'name': 'Z->qq',        'file_pattern': 'ZToQQ' },    \n",
    "    2: { 'name': 'TTbar->bqq',   'file_pattern': 'TTBar_' },\n",
    "    # 3: { 'name': 'Z->nunu',      'file_pattern': 'ZJetsToNuNu' },\n",
    "    # 4: { 'name': 'TTbar->blnu',  'file_pattern': 'TTBarLep' },\n",
    "    # 5: { 'name': 'H->bb',        'file_pattern': 'HToBB' },\n",
    "    # 6: { 'name': 'H->cc',        'file_pattern': 'HToCC' },\n",
    "    # 7: { 'name': 'H->gg',        'file_pattern': 'HToGG' },\n",
    "    # 8: { 'name': 'H->WW(2q1l)',  'file_pattern': 'HToWW2Q1L' },\n",
    "    # 9: { 'name': 'H->WW(4q)',    'file_pattern': 'HToWW4Q' }\n",
    "}\n",
    "\n",
    "def get_label_from_filename(fname):\n",
    "    for label, info in classes.items():\n",
    "        if info['file_pattern'] in fname:\n",
    "            return label\n",
    "    return -1\n",
    "\n",
    "# Loader: load jets from jetclass/ (ROOT files)\n",
    "def load_jets_from_inputs(max_jets_per_file=200, max_particles=128):\n",
    "    # Look in jetclass/ directory\n",
    "    files = glob.glob('jetclass/*.root')\n",
    "    jets_X = []\n",
    "    jets_w = []\n",
    "    labels = []\n",
    "    \n",
    "    if not files:\n",
    "        print(\"WARNING: No .root files found in jetclass/ directory!\")\n",
    "        return [], [], []\n",
    "\n",
    "    print(f\"Found {len(files)} ROOT files. Loading...\")\n",
    "\n",
    "    for fpath in files:\n",
    "        fname = os.path.basename(fpath)\n",
    "        lab = get_label_from_filename(fname)\n",
    "        \n",
    "        if lab == -1:\n",
    "            # print(f\"Skipping file with unknown class: {fname}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Loading {classes[lab]['name']} from {fname}...\")\n",
    "\n",
    "        try:\n",
    "            with uproot.open(fpath) as root:\n",
    "                # Search for any tree and examine its branches\n",
    "                for key in root.keys():\n",
    "                    try:\n",
    "                        obj = root[key]\n",
    "                        if hasattr(obj, 'arrays'):\n",
    "                            arrs = obj.arrays(library='np')\n",
    "                            keys = arrs.keys()\n",
    "                            \n",
    "                            # Check for px, py, pz\n",
    "                            px_key = next((k for k in keys if k.endswith('px')), None)\n",
    "                            py_key = next((k for k in keys if k.endswith('py')), None)\n",
    "                            pz_key = next((k for k in keys if k.endswith('pz')), None)\n",
    "                            \n",
    "                            if px_key and py_key and pz_key:\n",
    "                                px = arrs[px_key]\n",
    "                                py = arrs[py_key]\n",
    "                                pz = arrs[pz_key]\n",
    "                                n_jets = min(len(px), max_jets_per_file)\n",
    "                                for i in range(n_jets):\n",
    "                                    pxi = np.array(px[i])[:max_particles]\n",
    "                                    pyi = np.array(py[i])[:max_particles]\n",
    "                                    pzi = np.array(pz[i])[:max_particles]\n",
    "                                    \n",
    "                                    # Filter empty jets or jets with very low pT\n",
    "                                    if len(pxi) < 2: continue\n",
    "                                    \n",
    "                                    pti = np.sqrt(pxi**2 + pyi**2)\n",
    "                                    if pti.sum() < 1e-3: continue\n",
    "\n",
    "                                    p_mag = np.sqrt(pxi**2 + pyi**2 + pzi**2)\n",
    "                                    denom = p_mag - pzi + 1e-12\n",
    "                                    eta = 0.5 * np.log((p_mag + pzi) / denom)\n",
    "                                    phi = np.arctan2(pyi, pxi)\n",
    "                                    \n",
    "                                    # Center the jet\n",
    "                                    eta_avg = np.average(eta, weights=pti)\n",
    "                                    phi_avg = np.average(phi, weights=pti)\n",
    "                                    eta -= eta_avg\n",
    "                                    phi -= phi_avg\n",
    "                                    phi = (phi + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "                                    X = np.stack([eta, phi], axis=1)\n",
    "                                    w = pti / pti.sum()\n",
    "                                    \n",
    "                                    jets_X.append(X)\n",
    "                                    jets_w.append(w)\n",
    "                                    labels.append(lab)\n",
    "                                break\n",
    "                            \n",
    "                            # Check for pt, eta, phi\n",
    "                            pt_key = next((k for k in keys if k.endswith('pt')), None)\n",
    "                            eta_key = next((k for k in keys if k.endswith('eta')), None)\n",
    "                            phi_key = next((k for k in keys if k.endswith('phi')), None)\n",
    "\n",
    "                            if pt_key and eta_key and phi_key:\n",
    "                                pt = arrs[pt_key]\n",
    "                                eta = arrs[eta_key]\n",
    "                                phi = arrs[phi_key]\n",
    "                                n_jets = min(len(pt), max_jets_per_file)\n",
    "                                for i in range(n_jets):\n",
    "                                    pti = np.array(pt[i])[:max_particles]\n",
    "                                    etai = np.array(eta[i])[:max_particles]\n",
    "                                    phii = np.array(phi[i])[:max_particles]\n",
    "                                    \n",
    "                                    if len(pti) < 2: continue\n",
    "                                    if pti.sum() < 1e-3: continue\n",
    "\n",
    "                                    # Center\n",
    "                                    eta_avg = np.average(etai, weights=pti)\n",
    "                                    phi_avg = np.average(phii, weights=pti)\n",
    "                                    etai -= eta_avg\n",
    "                                    phii -= phi_avg\n",
    "                                    phii = (phii + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "                                    X = np.stack([etai, phii], axis=1)\n",
    "                                    w = pti / pti.sum()\n",
    "                                    \n",
    "                                    jets_X.append(X)\n",
    "                                    jets_w.append(w)\n",
    "                                    labels.append(lab)\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    return jets_X, jets_w, np.array(labels)\n",
    "\n",
    "# Load Data\n",
    "jets_X, jets_w, labels = load_jets_from_inputs(max_jets_per_file=1000)\n",
    "N = len(jets_X)\n",
    "print(f\"Total jets loaded: {N}\")\n",
    "\n",
    "if N == 0:\n",
    "    raise ValueError(\"No jets loaded! Please check jetclass/ directory.\")\n",
    "\n",
    "# OT computation: Toggle between Sinkhorn (approximate) and EnergyFlow (exact)\n",
    "USE_SINKHORN = True  # Default to Sinkhorn for speed\n",
    "USE_GPU_IF_AVAILABLE = True\n",
    "\n",
    "def compute_pairwise_matrix_gpu(jets_X, jets_w, max_particles=128):\n",
    "    \"\"\"\n",
    "    Compute pairwise Sinkhorn distances using GeomLoss on GPU.\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for GPU...\")\n",
    "    N = len(jets_X)\n",
    "    \n",
    "    # Pad jets to fixed size\n",
    "    X_pad = np.zeros((N, max_particles, 2), dtype=np.float32)\n",
    "    w_pad = np.zeros((N, max_particles), dtype=np.float32)\n",
    "    \n",
    "    for i in range(N):\n",
    "        n_p = min(len(jets_X[i]), max_particles)\n",
    "        X_pad[i, :n_p] = jets_X[i][:n_p]\n",
    "        w_pad[i, :n_p] = jets_w[i][:n_p]\n",
    "        # Normalize weights again just in case\n",
    "        w_sum = w_pad[i].sum()\n",
    "        if w_sum > 0:\n",
    "            w_pad[i] /= w_sum\n",
    "\n",
    "    # Convert to Torch tensors\n",
    "    X_dev = torch.tensor(X_pad).cuda()\n",
    "    w_dev = torch.tensor(w_pad).cuda()\n",
    "    \n",
    "    # Define GeomLoss Sinkhorn\n",
    "    # blur = 0.05 corresponds to the regularization parameter\n",
    "    loss = geomloss.SamplesLoss(\"sinkhorn\", p=2, blur=0.05)\n",
    "    \n",
    "    print(\"Computing pairwise matrix on GPU (this may take a moment)...\")\n",
    "    \n",
    "    D_matrix = np.zeros((N, N))\n",
    "    \n",
    "    # Batch size for rows. \n",
    "    # We compute distances from 'batch_size' jets to ALL 'N' jets at once.\n",
    "    # Memory usage is roughly (batch_size * N) * (max_particles^2) * 4 bytes.\n",
    "    # For N=3000, P=128, batch_size=10 -> ~30k pairs -> ~2GB memory for kernel. Safe for T4.\n",
    "    batch_size = 10 \n",
    "    \n",
    "    for i in range(0, N, batch_size):\n",
    "        end = min(i + batch_size, N)\n",
    "        B_curr = end - i\n",
    "        \n",
    "        # Batch of rows: (B, P, D)\n",
    "        X_batch = X_dev[i:end]\n",
    "        w_batch = w_dev[i:end]\n",
    "        \n",
    "        # We need to compute B_curr * N pairwise distances.\n",
    "        # We construct flat tensors of size (B_curr * N, P, D)\n",
    "        \n",
    "        # Expand Source (i): Repeat each row N times\n",
    "        # (B, P, D) -> (B * N, P, D)\n",
    "        Xi_flat = X_batch.repeat_interleave(N, dim=0)\n",
    "        wi_flat = w_batch.repeat_interleave(N, dim=0)\n",
    "        \n",
    "        # Expand Target (j): Tile the full set of N columns B times\n",
    "        # (N, P, D) -> (B * N, P, D)\n",
    "        Xj_flat = X_dev.repeat(B_curr, 1, 1)\n",
    "        wj_flat = w_dev.repeat(B_curr, 1) # Fixed: repeat 2D tensor only once for dim 1\n",
    "        \n",
    "        # Compute loss\n",
    "        # Output: (B * N, )\n",
    "        # GeomLoss expects (Batch, P, D) inputs\n",
    "        L_flat = loss(wi_flat, Xi_flat, wj_flat, Xj_flat)\n",
    "        \n",
    "        # Reshape to (B, N)\n",
    "        L_mat = L_flat.view(B_curr, N)\n",
    "        \n",
    "        D_matrix[i:end, :] = L_mat.detach().cpu().numpy()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Computed rows {i} to {end} of {N}...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return D_matrix\n",
    "\n",
    "def compute_ot_pair_cpu(i, j):\n",
    "    Xi, wi = jets_X[i], jets_w[i]\n",
    "    Xj, wj = jets_X[j], jets_w[j]\n",
    "    \n",
    "    if USE_SINKHORN:\n",
    "        M = ot.dist(Xi, Xj, metric='euclidean')\n",
    "        reg = 0.05 \n",
    "        val = ot.sinkhorn2(wi, wj, M, reg)\n",
    "        if isinstance(val, np.ndarray): val = val.item()\n",
    "        return i, j, float(val)\n",
    "    else:\n",
    "        ev1 = np.column_stack((wi, Xi[:, 0], Xi[:, 1]))\n",
    "        ev2 = np.column_stack((wj, Xj[:, 0], Xj[:, 1]))\n",
    "        val = ef.emd.emd(ev1, ev2, R=10.0)\n",
    "        return i, j, float(val)\n",
    "\n",
    "# Main Computation Logic\n",
    "if USE_SINKHORN and USE_GPU and USE_GPU_IF_AVAILABLE:\n",
    "    print(\"Using GPU-accelerated Sinkhorn (GeomLoss)...\")\n",
    "    D = compute_pairwise_matrix_gpu(jets_X, jets_w)\n",
    "    method_name = \"Sinkhorn (GPU)\"\n",
    "else:\n",
    "    method_name = \"Sinkhorn (CPU)\" if USE_SINKHORN else \"Exact EMD (CPU)\"\n",
    "    print(f\"Computing pairwise {method_name} distances using CPU parallel processing...\")\n",
    "    \n",
    "    n_cores = min(8, multiprocessing.cpu_count())\n",
    "    pairs = [(i, j) for i in range(N) for j in range(i+1, N)]\n",
    "    print(f\"Total pairs to compute: {len(pairs)}\")\n",
    "\n",
    "    results = Parallel(n_jobs=n_cores, verbose=5)(\n",
    "        delayed(compute_ot_pair_cpu)(i, j) for i, j in pairs\n",
    "    )\n",
    "\n",
    "    D = np.zeros((N, N))\n",
    "    for i, j, val in results:\n",
    "        D[i, j] = val\n",
    "        D[j, i] = val\n",
    "    np.fill_diagonal(D, 0.0)\n",
    "\n",
    "print(\"Pairwise distance matrix computed.\")\n",
    "\n",
    "# t-SNE embedding using precomputed distances\n",
    "tsne = TSNE(n_components=2, metric='precomputed', init='random', random_state=42, perplexity=min(30, N-1))\n",
    "emb = tsne.fit_transform(D)\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "scatter = plt.scatter(emb[:, 0], emb[:, 1], c=labels, cmap='tab10', s=40, alpha=0.8)\n",
    "plt.title(f't-SNE embedding of jets ({method_name})')\n",
    "# Create legend handles manually since we might not have all classes\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab10(i), label=classes[i]['name'], markersize=10) for i in classes if i in np.unique(labels)]\n",
    "plt.legend(handles=handles)\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Scatter-thumbnail helper: draw a small jet (particle scatter) centered at (x0, y0)\n",
    "def plot_jet_scatter(ax, x0, y0, X, w, scale=0.25, max_markersize=60, cmap='viridis', alpha=0.8):\n",
    "    xs = x0 + X[:, 0] * scale\n",
    "    ys = y0 + X[:, 1] * scale\n",
    "    s = (w / (w.max() + 1e-12)) * max_markersize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
